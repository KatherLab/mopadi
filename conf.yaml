# Which model to train: the order is autoencoder -> latent DPM (optional, for unconditional images generation) -> linear classifier or MIL classifier
train_type: autoenc   # options: autoenc, latent_dpm (not implemented yet), linear_cls (not implemented yet)

# Training the autoencoder ideally requires 8 (40Gb) or 4 (80Gb) x A100, for all the others one local GPU is enough
gpus: [0, 1]

data:
  # Directories containing tiles; can be multiple cohorts; by default will get all files with any of these extensions: ['.jpg', '.jpeg', '.png', '.tif', '.tiff']
  data_dirs:
    - /mnt/bulk-mars/laura/diffae/data/texture100k/NCT-CRC-HE-100K
    #- path2

  # Optional, will not use these patients for training the autoencoder or will only use them if split is 'test'
  # For an example of such file, please check datasets/patient_splits/TCGA_CRC_test_split.json
  #test_patients_file_path: /path/to/test_patients.json

  # Split parameter determines whether tiles will be sampled from train set or test set (if test_patients_file is given); 
  # Possible options: none, train, test. If none, all the tiles in data_dirs will be used
  split: none

  # If one wants to limit the number of tiles for cohorts above the threshold
  #max_tiles_per_patient: 1000
  # Tile number limit after which the number of tiles per patient starts to be limited
  #cohort_size_threshold: 1400000

  # This enables processing zipped tiles outputted by STAMP, no need to extract them
  process_only_zips: false

  # This is useful in case multiple huge cohorts are used for the training and debugging needs to be done. Otherwise, can be ignored
  # To enable saving these cache files, uncomment and define the paths
  #cache_pickle_tiles_path: /path/to/cache.pkl
  #cache_cohort_sizes_path: /path/to/cohort_sizes_cache.json

  # If resizing is enabled, the tiles will be resized according to the given image size
  # otherwise please enter the actual tile size in pixels
  do_resize: false
  img_size: 224
  do_normalize: true   # for the diffusion process to work well images are normalized with mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)

exp:
  # Directory where training output will be saved, typically at checkpoints/EXPERIMENT_NAME
  base_dir: /mnt/bulk-mars/laura/diffae/mopadi/checkpoints/texture100k-trial

model:
  # Total samples - not the total number of tiles, but how many samples the model will see before the training 
  # is terminated, i.e., epoch number will depend on this number. Typically set high, and the training is stopped 
  # for examples after 72-98h on HPC when the FID score is low enough (<20 good; <10 excellent)
  total_samples: 200_000_000  # default: 200_000_000
  batch_size: 32              # default: 64
  batch_size_eval: 32         # default: batch_size_eval
  warmup: 0                   # default: warmup

  # All the other model parameters that can be modified can be found in the configs/config.py. 
  # Default values for different models can also be found in configs/templates.py, configs/templates_cls.py and configs/templates_latent.py
