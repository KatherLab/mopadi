# Training the autoencoder ideally requires 8 (40Gb) or 4 (80Gb) x A100, for all the other models one GPU is enough
# If given, GPUs will also be used for manipulation
# By default, if no gpus are given, the first GPU will be used for training if cuda is available. If needed, adjust this behaviour in run_mopadi.py
gpus: [0, 1]

exp:
  # Directory where training output will be saved, typically at checkpoints/EXPERIMENT_NAME
  base_dir: /mnt/bulk-mars/laura/diffae/mopadi/checkpoints/texture100k-trial2
    
train:
  # Which model to train: the order is autoencoder -> latent DPM (optional, for unconditional images generation) -> linear classifier or MIL classifier
  train_type: autoenc   # options: autoenc, latent_dpm, linear_cls (not implemented yet), mil_clf

  data:
    # Directories containing tiles; can be multiple cohorts; by default will get all files with any of these extensions: ['.jpg', '.jpeg', '.png', '.tif', '.tiff']
    data_dirs:
      - /mnt/bulk-mars/laura/diffae/data/texture100k/NCT-CRC-HE-100K
      #- path2

    # Optional, will not use these patients for training the autoencoder or will only use them if split is 'test'
    # For an example of such file, please check datasets/patient_splits/TCGA_CRC_test_split.json
    #test_patients_file_path: /path/to/test_patients.json

    # Split parameter determines whether tiles will be sampled from train set or test set (if test_patients_file is given); 
    # Possible options: none, train, test. If none, all the tiles in data_dirs will be used
    split: none

    # If one wants to limit the number of tiles for cohorts above the threshold
    #max_tiles_per_patient: 1000
    # Tile number limit after which the number of tiles per patient starts to be limited
    #cohort_size_threshold: 1400000

    # This enables processing zipped tiles outputted by STAMP, no need to extract them
    process_only_zips: false

    # This is useful in case multiple huge cohorts are used for the training and debugging needs to be done. Otherwise, can be ignored
    # To enable saving these cache files, uncomment and define the paths
    #cache_pickle_tiles_path: /path/to/cache.pkl
    #cache_cohort_sizes_path: /path/to/cohort_sizes_cache.json

    # If resizing is enabled, the tiles will be resized according to the given image size
    # otherwise please enter the actual tile size in pixels
    do_resize: false
    img_size: 224
    do_normalize: true   # for the diffusion process to work well images are normalized with mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)

  model:
    # All the parameters that can be modified can be found in the configs/config.py. 
    # Default values for different models can also be found in configs/templates.py, configs/templates_cls.py and configs/templates_latent.py

    # Total samples - not the total number of tiles, but how many samples the model will see before the training 
    # is terminated, i.e., epoch number will depend on this number. Typically set high, and the training is stopped 
    # for examples after 72-98h on HPC when the FID score is low enough (<20 good; <10 excellent)
    total_samples: 200 #_000_000   # default: 200_000_000 (autoencoder), 130_000_000 (latent DPM), 300_000 (linear classifier)

    # Number of samples the model 'sees' before the evaluation step is performed (LPIPS, FID is computed)
    eval_every_samples: 40       # default: 1_000_000 (autoencoder)        
    eval_ema_every_samples: 40   # default: 1_000_000 (autoencoder)   

    # Select the batch size according to the available GPU memory to avoid OOM errors, but high enough to efficiently utilize the GPU
    # The batch size is divided by the number of GPUs, so if you have 4 GPUs and set batch_size=64, each GPU will see 16 samples
    batch_size: 16               # default: 64 (autoencoder & latent DPM), 32 (linear classifier)
    batch_size_eval: 16          # default: 32
    warmup: 0                     # default: 0

manipulate:
  
