# The order of training mopadi models: 
# autoencoder -> latent DPM (optional, for unconditional synthetic image generation) -> MIL classifier

# After all the steps, the resulting folder structure will look like this:
# checkpoints/
# └── experiment_name/                         # base_dir
#     ├── autoenc/                             # Autoencoder output
#     ├── latent_dpm/                          # Latent DPM (optional)
#     ├── mil_classifier_TARGET_label/         # MIL classifier
#     │   ├── crossval/                        # Cross-validation results (optional)
#     │   │   ├── fold_0/
#     │   │   ├── ...
#     │   │   └── fold_n/
#     │   ├── full_model/                      # Model trained on all data
#     │   └── counterfactuals/                 # Counterfactual results using the full model
# Only base_dir is required, the rest will be created automatically

# Training the autoencoder ideally requires 8 (40Gb) or 4 (80Gb) x A100, for all the other models one GPU is enough
# For manipulation, only one GPU is needed
# By default, if no gpus are given, the first GPU will be used for training if cuda is available. If needed, adjust this behaviour in run_mopadi.py
gpus: [0]
# Directory where training output will be saved, typically at checkpoints/EXPERIMENT_NAME
# The folders for autoencoder, latent DPM and classifiers will be created automatically under this directory
base_dir: checkpoints/experiment_name


data:
  # Necessary for the autoencoder (step 1) and latent DPM training (optional step 2)
  # Directories containing .tar shards; can be multiple cohorts
  data_dirs:
    - /path/to/tar/shards/folder
    #- /can/also/be/glob/pattern/for/specific/cohort/like/this/TCGA-CRC-*.tar

  feature_dirs:
    - /path/to/folder/with/h5/feature/files

  # Name of the feature extractor used to extract features; options: conch, conch1.5, virchow2, uni2
  feature_extractor: conch

  # If one wants to limit the number of tiles for cohorts above the threshold
  #max_tiles_per_patient: 1000
  # Tile number limit after which the number of tiles per patient starts to be limited
  #cohort_size_threshold: 1400000

  # If resizing is enabled, the tiles will be resized according to what a given feature extractor expects:
  # Conch:      448x448 px
  # Conch 1.5:  448x448 px
  # Virchow2:   224x224 px
  # UNI2:       224x224 px
  do_resize: true


# STEP 1: AUTOENCODER
autoenc_model:
  # All the parameters that can be modified can be found in the configs/config.py. 
  # More default values can be found in configs/templates.py

  # Total samples - not the total number of tiles, but how many samples the model will see before the training 
  # is terminated, i.e., epoch number will depend on this number. Typically set high, and the training is stopped 
  # for examples after 72-98h on HPC when the FID score is low enough (<20 good; <10 excellent)
  total_samples: 200000000

  # Number of samples the model 'sees' before the evaluation step is performed (LPIPS, FID is computed)
  eval_every_samples: 1000000
  eval_ema_every_samples: 1000000

  # Select the batch size according to the available GPU memory to avoid OOM errors, but high enough to efficiently utilize the GPU
  # The batch size is divided by the number of GPUs, so if you have 4 GPUs and set batch_size=64, each GPU will see 16 samples
  #batch_size: 64
  #batch_size_eval: 64

  # How many images are generated during logging, reduce if you run out of GPU memory
  #sample_size: 32


# STEP 2 (optional): LATENT DIFFUSION PROBABILISTIC MODEL (for unconditional synthetic image generation)
latent_dpm:
  # More default values can be found in configs/templates_latent.py

  # Total samples - not the total number of tiles, but how many samples the model will see before the training 
  # is terminated, i.e., epoch number will depend on this number
  total_samples: 130000000
  sample_every_samples: 4000000
  steps_per_epoch: 5000

  #batch_size: 124
  #batch_size_eval: 32


# STEP 3: MIL CLASSIFIER (needed for the manipulation of images)
mil_classifier:
    # Path to extracted feature files (h5 format) for training
    feat_path_train: /path/to/extracted/features/of/train/set
    # Path to extracted feature files (h5 format) for testing
    feat_path_test: /path/to/extracted/features/of/test/set
    # Clinical table with ground truth data, must contain PATIENT column
    clini_table: /path/to/clini_table.csv   # or .xlsx
    # Target label column in clinical table
    target_label: TARGET
    # If values of target label column are not numerical (e.g., 0, 1), provide a dictionary with mapping
    #target_dict: {"CLASS1": 0, "CLASS2": 1}
    # How to split the filename to get patient name; e.g., TCGA-AB-1234-12345678.tif if fnames_index = 3 -> TCGA-AB-1234
    #fname_index: 3
    # Number of folds for cross-validation
    number_of_folds: 5

    nr_feats: 512
    num_epochs: 100

    # The rest of parameters are needed only for the manipulation (creation of counterfactuals)

    # Directory to the test tiles folder; must match features in feat_path_test
    # alternatively, one can use the same folder as for training (data_dirs), but with the split set to 'test' & test_patients_file_path specified
    #images_dir: /path/to/test/tiles/folder
    # Make sure features extracted from images in images_dir are in feat_path_test folder

    # If the following (patients/filename) are not specified, all the images in the directory will be manipulated
    #patients: 
    #  - patientID1
    #  - patientID2
    #filename: example  # NOTE: without the extension

    # How many top predictive tiles per patient to use for manipulation if filename and patients are not specified
    nr_top_tiles: 5

    # Recommended starting point values for MIL approach, depending on the results (whether the class flips) might need to be adjusted
    manipulation_levels: 
      - 0.02
      - 0.04
      - 0.06
      - 0.08

    # Whether to use the pretrained model (will download from Hugging Face both the autoencoder and the classifier)
    use_pretrained: false
    #pretrained_autoenc_name: CHOOSE_FROM_OPTIONS    # Options: brca_512_model, crc_512_model, pancancer_model
    #pretrained_clf_name: CHOOSE_FROM_OPTIONS        # Options: msi (default for crc_512_model), type or e2_center (for brca_512_model), lung or liver (for pancancer_model)
